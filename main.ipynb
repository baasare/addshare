{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.keras.backend as K\n",
    "\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.asymmetric import ec\n",
    "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
    "from cryptography.hazmat.primitives.ciphers.aead import AESGCM\n",
    "\n",
    "from helpers.utils import get_public_key, get_private_key, NumpyEncoder, NumpyDecoder, get_dataset, fetch_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T19:57:09.569444Z",
     "start_time": "2024-01-22T19:57:03.340393Z"
    }
   },
   "id": "694361724734c663"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def encrypt_message(message, recipient_public_key):\n",
    "    ephemeral_private_key = ec.generate_private_key(ec.SECP256R1(), default_backend())\n",
    "    ephemeral_public_key = ephemeral_private_key.public_key()\n",
    "\n",
    "    shared_secret = ephemeral_private_key.exchange(ec.ECDH(), recipient_public_key)\n",
    "\n",
    "    derived_key_material = HKDF(\n",
    "        algorithm=hashes.SHA256(),\n",
    "        length=32 + 12,\n",
    "        salt=None,\n",
    "        info=b'',\n",
    "    ).derive(shared_secret)\n",
    "\n",
    "    encryption_key = derived_key_material[:32]\n",
    "    nonce = derived_key_material[32:]\n",
    "\n",
    "    cipher = AESGCM(encryption_key)\n",
    "    ciphertext = cipher.encrypt(nonce, message.encode(), None)\n",
    "\n",
    "    return ephemeral_public_key, ciphertext\n",
    "\n",
    "\n",
    "def decrypt_message(ciphertext, ephemeral_public_key, recipient_private_key):\n",
    "    shared_secret = recipient_private_key.exchange(ec.ECDH(), ephemeral_public_key)\n",
    "\n",
    "    derived_key_material = HKDF(\n",
    "        algorithm=hashes.SHA256(),\n",
    "        length=32 + 12,\n",
    "        salt=None,\n",
    "        info=b'',\n",
    "    ).derive(shared_secret)\n",
    "\n",
    "    encryption_key = derived_key_material[:32]\n",
    "    nonce = derived_key_material[32:]\n",
    "\n",
    "    cipher = AESGCM(encryption_key)\n",
    "    decrypted_message = cipher.decrypt(nonce, ciphertext, None)\n",
    "\n",
    "    return decrypted_message.decode()\n",
    "\n",
    "# \n",
    "# recipient_private_key = ec.generate_private_key(ec.SECP256R1(), default_backend())\n",
    "# recipient_public_key = recipient_private_key.public_key()\n",
    "# \n",
    "# message_to_encrypt = \"Hello, this is plaintext\"\n",
    "# \n",
    "# ephemeral_public_key, ciphertext = encrypt_message(message_to_encrypt, recipient_public_key)\n",
    "# \n",
    "# print(\"Ciphertext:\", ciphertext)\n",
    "# \n",
    "# decrypted_message = decrypt_message(ciphertext, ephemeral_public_key, recipient_private_key)\n",
    "# print(\"Decrypted Message:\", decrypted_message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T15:37:58.873997Z",
     "start_time": "2024-01-12T15:37:58.868863Z"
    }
   },
   "id": "46b937130a47da21"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 2s 71ms/step - loss: 1.1147 - accuracy: 0.6400\n"
     ]
    }
   ],
   "source": [
    "indexes = fetch_index(\"mnist\")\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, y_train, x_test, y_test = get_dataset(indexes[0], \"mnist\", x_train, y_train, x_test, y_test)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), input_shape=(28, 28, 1), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=1, batch_size=100, verbose=True)  #  history.history['loss']\n",
    "loss_perturbed = model.evaluate(x_test, y_test, verbose=0)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T18:08:39.861512Z",
     "start_time": "2024-01-12T18:08:35.409848Z"
    }
   },
   "id": "1d2aa128291fa20e"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "json_str = json.dumps(model.get_weights(), cls=NumpyEncoder)\n",
    "\n",
    "private_key = get_private_key(1, 'elliptical')\n",
    "public_key = get_public_key(1, 'elliptical')\n",
    "\n",
    "ephemeral_public_key, ciphertext = encrypt_message(json_str, public_key)\n",
    "decrypted_message = decrypt_message(ciphertext, ephemeral_public_key, private_key)\n",
    "decrypted_weights = json.loads(decrypted_message, cls=NumpyDecoder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-12T16:08:17.045869Z"
    }
   },
   "id": "9939794e5f4d26ce"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def get_logits(model, x):\n",
    "    return model(x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def loss_fn(logits, y):\n",
    "    return K.categorical_crossentropy(y, logits)\n",
    "\n",
    "\n",
    "def random_weight_selection(weights, fraction=0.25):\n",
    "    percentage = max(0, min(100, fraction))\n",
    "    flattened_weights = weights.flatten()\n",
    "    num_elements = int(np.ceil(percentage * flattened_weights.size))\n",
    "    indexes = np.random.choice(flattened_weights.size, size=num_elements, replace=False)\n",
    "    original_indices = np.unravel_index(indexes, weights.shape)\n",
    "    indices = [arr.tolist() for arr in original_indices]\n",
    "    return indices\n",
    "\n",
    "\n",
    "def magnitude_weight_selection(array, percentage=0.25):\n",
    "    percentage = max(0, min(100, percentage))\n",
    "    num_elements = int(np.ceil(percentage / 100 * array.size))\n",
    "    indices_of_largest = np.argpartition(array.flatten(), -num_elements)[-num_elements:]\n",
    "    original_indices = np.unravel_index(indices_of_largest, array.shape)\n",
    "    indices = [arr.tolist() for arr in original_indices]\n",
    "    return indices\n",
    "\n",
    "\n",
    "def obd_weight_selection(model, x, y, weights, fraction):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(y, predictions)\n",
    "        gradients = tape.gradient(loss, weights)\n",
    "        sensitivities = [grad * weight for grad, weight in zip(gradients, weights)]\n",
    "        sensitivities = np.array(sensitivities)\n",
    "        return magnitude_weight_selection(sensitivities, fraction)\n",
    "    \n",
    "def regularization_weight_selection(model, x, y, reg_type, weights, fraction):\n",
    "    regularization_lambda = 0.01\n",
    "    l1_regularization = regularization_lambda * tf.reduce_sum(tf.abs(weights))\n",
    "    l2_regularization = regularization_lambda * tf.reduce_sum(tf.square(weights))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(y, predictions)\n",
    "        gradients = tape.gradient(loss, weights)\n",
    "\n",
    "    total_gradient = gradients + (l1_regularization if reg_type == \"l1\" else l2_regularization)\n",
    "    l1_weight = np.array(weights - total_gradient)\n",
    "    return magnitude_weight_selection(l1_weight, fraction)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-10T16:47:42.460729Z"
    }
   },
   "id": "29ebbf1604008e33"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "num_servers = 3\n",
    "highest_range = np.finfo('float16').max\n",
    "\n",
    "all_servers = []\n",
    "servers_model = []\n",
    "\n",
    "for server_index in range(num_servers):\n",
    "    all_servers.append({})\n",
    "    servers_model.append({})\n",
    "\n",
    "layer_dict, layer_shape, shares_dict = {}, {}, {}\n",
    "data = np.array([1, 2, 3])\n",
    "no_of_layers = len(data)\n",
    "for layer_index in range(no_of_layers):\n",
    "    layer_dict[layer_index] = data[layer_index]\n",
    "    layer_shape[layer_index] = data[layer_index].shape\n",
    "\n",
    "for layer_index in range(no_of_layers):\n",
    "    shares_dict[layer_index] = np.zeros(shape=(num_servers,) + layer_shape[layer_index], dtype=np.float64)\n",
    "\n",
    "    for server_index in range(num_servers - 1):\n",
    "        shares_dict[layer_index][server_index] = np.random.uniform(low=-highest_range, high=highest_range,\n",
    "                                                                   size=layer_shape[layer_index]).astype(np.float64)\n",
    "\n",
    "    share_sum_except_last = np.array(shares_dict[layer_index][:num_servers - 1]).sum(axis=0,\n",
    "                                                                                            dtype=np.float64)\n",
    "    x = np.copy(np.array(layer_dict[layer_index], dtype=np.float64))\n",
    "    last_share = np.subtract(x, share_sum_except_last, dtype=np.float64)\n",
    "    shares_dict[layer_index][num_servers - 1] = last_share\n",
    "\n",
    "for server_index in range(num_servers):\n",
    "    for layer_index in range(len(shares_dict)):\n",
    "        all_servers[server_index][layer_index] = shares_dict[layer_index][server_index]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T17:19:01.315624Z",
     "start_time": "2024-01-19T17:19:01.303360Z"
    }
   },
   "id": "a66da73497186683"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def generate_integer_additive_shares(value, n):\n",
    "    arr = np.asarray(value)\n",
    "    rand_arr = np.random.randint(1000, size=(n - 1,) + arr.shape)\n",
    "    shares = np.concatenate((rand_arr, [arr - rand_arr.sum(axis=0)]), axis=0)\n",
    "    return shares\n",
    "\n",
    "\n",
    "def f_to_i(x, scale=1 << 32):\n",
    "    if x < 0:\n",
    "        if pow(2, 64) - (abs(x) * scale) > (pow(2, 64) - 1):\n",
    "            return np.uint64(0)\n",
    "        x = pow(2, 64) - np.uint64(abs(x) * scale)\n",
    "    else:\n",
    "        x = np.uint64(scale * x)\n",
    "    return np.uint64(x)\n",
    "\n",
    "\n",
    "def i_to_f(x, scale=1 << 32):\n",
    "    l = 64\n",
    "    t = x > (pow(2, (l - 1)) - 1)\n",
    "    if t:\n",
    "        x = pow(2, l) - x\n",
    "        y = np.uint64(x)\n",
    "        y = np.float32(y * (-1)) / scale\n",
    "    else:\n",
    "        y = np.float32(np.uint64(x)) / scale\n",
    "    return y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T19:57:09.585226Z",
     "start_time": "2024-01-22T19:57:09.579619Z"
    }
   },
   "id": "3c1e029e9f56ab74"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "f_to_i_v = np.vectorize(f_to_i)\n",
    "i_to_f_v = np.vectorize(i_to_f)\n",
    "\n",
    "x_weights = np.array([-1.6, 4.7, 0.3])\n",
    "x_weights_int = f_to_i_v(x_weights)\n",
    "shares_int = generate_integer_additive_shares(x_weights_int, 3)\n",
    "x_weights_int_assembled = np.sum(shares_int, axis=0, keepdims=True)\n",
    "x_weights_float_assembled = np.round(i_to_f_v(x_weights_int_assembled), 3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T19:57:09.608942Z",
     "start_time": "2024-01-22T19:57:09.592592Z"
    }
   },
   "id": "64ccd2e7f474e34b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6  4.7  0.3]\n",
      "[[-1.6  4.7  0.3]]\n"
     ]
    }
   ],
   "source": [
    "print(x_weights)\n",
    "print(x_weights_float_assembled)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T19:57:09.611640Z",
     "start_time": "2024-01-22T19:57:09.605193Z"
    }
   },
   "id": "4dd52bb2dafe93f6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    }
   ],
   "source": [
    "print(sum([200 / 5] * 5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T20:24:49.722709Z",
     "start_time": "2024-01-22T20:24:49.714989Z"
    }
   },
   "id": "8bf7c9d948d2b082"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "309529e22b4bf556"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
